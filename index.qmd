---
title: 'Reliable maintenance of machine learning models'
format:
  revealjs: 
    theme: [dark, custom.scss]
    footer: <https://juliasilge.github.io/ml-maintenance-2023>
    preview-links: auto
    width: 1280
    height: 720
    incremental: true
    code-line-numbers: false
    highlight-style: a11y-light
    title-slide-attributes: 
      data-background-image: images/end_paper.jpg
      data-background-opacity: "0.2"
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"
---

## Hello!

<center>

<img src="https://github.com/juliasilge.png" style="border-radius: 50%;" width="300px"/>

[{{< fa brands github >}} \@juliasilge](https://github.com/juliasilge)

[{{< fa brands mastodon >}} \@juliasilge\@fosstodon.org](https://fosstodon.org/@juliasilge)

[{{< fa brands youtube >}} youtube.com/juliasilge](https://www.youtube.com/juliasilge)

[{{< fa link >}} juliasilge.com](https://juliasilge.com/)

</center>

#  {background-image="images/mohammed-gadi-2qCX0SsSwzs-unsplash.jpg"}

:::footer
:::

# Maintaining ML models is NOTHING LIKE THIS  {background-image="images/mohammed-gadi-2qCX0SsSwzs-unsplash.jpg" background-opacity=0.2}

#  {background-image="images/image-from-rawpixel-id-8863371-original.jpg"}

:::footer
:::

::: {.notes}
Never-ending laundry process
:::

# Maintaining ML models is never done {background-image="images/image-from-rawpixel-id-8863371-original.jpg" background-opacity=0.4}

:::footer
:::

::: {.notes}
New load of laundry -- when and how to retrain model

All software products need to be maintained, but models are both a software artifact *and* a statistical artifact
:::

# Both software and statistical products {background-image="images/image-from-rawpixel-id-8863371-original.jpg" background-opacity=0.4}

:::footer
:::

::: {.notes}
Unlike something that is "only" a software product, you could, say, retrain your model with new data and change its statistical properties without changing its software properties
:::

# What does performance mean? {background-image="images/end_paper.jpg" background-opacity=0.2}

::: {.notes}
When we talk about software or statistical properties, we're really talking about performance

One of the most basic prerequisites for model maintenance is monitoring the model's performance

Measuring model performance allows us to say:
:::

## My model is performing well!

. . .

üë©üèº‚Äçüîß My model returns predictions quickly, doesn't use too much memory or processing power, and doesn't have outages.


. . .

::: {.callout-caution icon=false}

## Metrics

::: {.nonincremental}
- latency
- memory and CPU usage
- uptime
:::

:::


## My model is performing well! 

. . .

üë©üèΩ‚Äçüî¨ My model returns predictions that are close to the true values for the predicted quantity.

. . .

::: {.callout-caution icon=false}

## Metrics

::: {.nonincremental}
- accuracy
- ROC AUC
- F1 score
- RMSE
- log loss
:::

:::

::: {.notes}
As data practitioners, we focus on monitoring statistical performance
:::

# Failures in statistical performance can be *silent* {background-image="images/image-from-rawpixel-id-8863371-original.jpg" background-opacity=0.4}

::: {.notes}
Statistical properties of a model have unique tendencies toward silence
:::

# MODEL DRIFT {background-image="images/end_paper.jpg" background-opacity=0.2}

:::{.notes}
data drift + concept drift
:::

# DATA DRIFT {background-image="images/end_paper.jpg" background-opacity=0.2}

::: {.notes}
Let's say I run a laundry service, and I have built a model to predict someone's likelihood to become my customer

I use a bunch of predictors, like how many loads of laundry they need to do each month, their income, whether they live in a house or apartment, and a bunch of other things

Data drift is about drift or change in these inputs, compared to what we had when we trained the model.

Very concretely, we measure this through summary statistics, through visualization
:::

# Monitor your inputs {background-color="white"}

```{r}
#| echo: false
#| fig-align: center
library(tidymodels)
theme_set(silgelib::theme_roboto())
data(diamonds)
diamond_split <- diamonds |> filter(x > 0.2) |> initial_split()
diamond_train <- training(diamond_split)
diamond_test <- testing(diamond_split)

p <- 
  ggplot(diamond_test, aes(x)) +
  geom_density(adjust = 2, linewidth = 1.2, alpha = 0.4, fill = "#e7ad52", color = "#e7ad52") +
  labs(x = "one of your predictors") +
  scale_x_continuous(limits = c(3, 12)) +
  scale_y_continuous(limits = c(NA, 0.4))

p
```


# Monitor your inputs {background-color="white"}

```{r}
#| echo: false
#| fig-align: center
p + geom_density(data = diamond_train,
                 adjust = 2, linewidth = 1.2, alpha = 0.4, fill = "#011520", color = "#011520")
```

::: {.notes}
We can make this kind of comparison visually, or we can use a statistical test
:::

# Monitor your inputs {background-color="white"}

```{r}
#| echo: false
#| fig-align: center
p + geom_density(data = diamond_train, aes(15.3 - x),
                 adjust = 2, linewidth = 1.2, alpha = 0.4, fill = "#011520", color = "#011520")
```

::: {.notes}
True/real changes in distribution vs. changes in data collection or similar
:::

# DATA DRIFT {background-image="images/end_paper.jpg" background-opacity=0.2}

Monitor your **inputs**


# CONCEPT DRIFT {background-image="images/end_paper.jpg" background-opacity=0.2}

. . .

Monitor your **outputs**

::: {.notes}
Relationship between the input and output is changing over time

In the city where I run a laundry service, the new apartments that are being built all have laundry in people's units
:::


## Monitor your outputs {background-color="white"}

```{r}
#| echo: false
data(two_class_example)
pretend_dates <- as.Date("2023-03-05") + 0:100
set.seed(2023)
laundry_service_monitoring <-
  as_tibble(two_class_example) |> 
  mutate(date = sample(pretend_dates, size = nrow(two_class_example), replace = TRUE)) |> 
  rename(.pred_customer = Class1, customer = truth, .pred = predicted) |> 
  arrange(date)
```


```{r}
library(vetiver)

laundry_service_monitoring |> 
  vetiver_compute_metrics(date, "week", customer, .pred)
```

## Monitor your outputs {background-color="white"}

```{r}
#| fig-align: center
#| output-location: fragment
library(vetiver)

laundry_service_monitoring |> 
  vetiver_compute_metrics(date, "week", customer, .pred) |> 
  vetiver_plot_metrics()
```

## Feedback loops üîÅ

Deployment of an ML model may *cause* data and/or concept drift

. . .

::: {.callout-caution icon=false}

## Examples

- Movie recommendation systems on Netflix, Disney+, Hulu
- Identifying fraudulent credit card transactions at Stripe
- Predictive policing models

:::


::: footer
[*Building Machine Learning Pipelines* by Hapke & Nelson](https://www.oreilly.com/library/view/building-machine-learning/9781492053187/)
:::

::: {.notes}
Users take some action as a result of a prediction, users rate or correct the quality of a prediction, produce feedback automatically
:::

# Stages of model monitoring maturity


1. Manual üôÇ

2. Reproducible ü§ì

3. Automated ü§©

::: footer
[5 Levels of MLOps Maturity](https://www.nannyml.com/blog/5-levels-of-mlops-maturity)
:::

## {background-color="white"}

![](images/dashboard1.png){fig-align="center"}

:::footer
:::

## {background-color="white"}

![](images/dashboard2.png){fig-align="center"}

:::footer
:::

## {background-color="white"}

![](images/dashboard3.png){fig-align="center"}

:::footer
:::

# Resilient models that are successful in the long term {background-image="images/end_paper.jpg" background-opacity=0.2}

::: {.notes}
We've talked about 

- understanding what performance means for models 
- how to consider both the software and statistical characteristics of a model
- measuring inputs and outputs to get a handle on drift

These are what you need to have a model that is easier to maintain
:::


## Learn more {background-image="images/image-from-rawpixel-id-8863371-original.jpg" background-opacity=0.3}

::: {.nonincremental}
-   Documentation at <https://vetiver.rstudio.com/>

-   [Webinar by Isabel Zimmerman and me](https://juliasilge.github.io/mlops-rstudio-meetup/) for Posit Enterprise Meetup

-   End-to-end demos from Posit Solution Engineering in [R](https://github.com/sol-eng/bike_predict) and [Python](https://github.com/sol-eng/bike_predict_python)
:::


## Thank you! {background-image="images/image-from-rawpixel-id-8863371-original.jpg" background-opacity=0.3}

<center>

<img src="https://github.com/juliasilge.png" style="border-radius: 50%;" width="300px"/>

[{{< fa brands github >}} \@juliasilge](https://github.com/juliasilge)

[{{< fa brands mastodon >}} \@juliasilge\@fosstodon.org](https://fosstodon.org/@juliasilge)

[{{< fa brands youtube >}} youtube.com/juliasilge](https://www.youtube.com/juliasilge)

[{{< fa link >}} juliasilge.com](https://juliasilge.com/)

</center>

::: footer
Image credits: [Mohammed Gadi](https://unsplash.com/photos/2qCX0SsSwzs), [Albert Edelfelt](https://www.rawpixel.com/image/8863371/image-art-public-domain-woman), [Bergen Public Libray](https://www.flickr.com/photos/bergen_public_library/sets/72157633827993925)
:::
