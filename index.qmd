---
title: 'Reliable maintenance of machine learning models'
format:
  revealjs: 
    footer: <https://juliasilge.github.io/ml-maintenance-2023>
    preview-links: auto
    width: 1280
    height: 720
    incremental: true
    code-line-numbers: false
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"
---

## Hello!

. . .

<center>

<img src="https://github.com/juliasilge.png" style="border-radius: 50%;" width="300px"/>

[{{< fa brands github >}} \@juliasilge](https://github.com/juliasilge)

[{{< fa brands mastodon >}} \@juliasilge\@fosstodon.org](https://fosstodon.org/@juliasilge)

[{{< fa brands youtube >}} youtube.com/juliasilge](https://www.youtube.com/juliasilge)

[{{< fa link >}} juliasilge.com](https://juliasilge.com/)

</center>

#  {background-image="images/mohammed-gadi-2qCX0SsSwzs-unsplash.jpg"}

# Maintaining ML models is NOTHING LIKE THIS  {background-image="images/mohammed-gadi-2qCX0SsSwzs-unsplash.jpg" background-opacity=0.5}

##  {background-image="images/image-from-rawpixel-id-8863371-original.jpg"}


## Outline

- Hook: unpacking suitcase vs. doing laundry
  - All software products needs to be maintained
  - Model retraining
  - Maintenance for a model typically means monitoring its performance
- What does performance mean?
  - ML model as a software artifact vs. a statistical artifact
  - "We aren't even talking about the same thing"
  - As data practitioners, we focus on monitoring statistical performance
  - Failures in statistical performance will more often be _silent_ (how bad can this be? very bad)
- Model drift == data drift and/or concept drift
- Data drift == statistical distribution of input feature changes
  - About the **inputs**
  - What can you monitor?
    - Statistical distribution of features individually
    - Statistical characteristics of features as a whole
  - True/real changes in distribution vs. changes in data collection or similar  
  - Example: 
    - Show EDA + tidymodels on model trust
- Concept drift == change in the relationship between input features and outcome
  - About the relationship between **inputs** and **outputs**
  - What can you monitor?
    - Model performance metrics, like RMSE or accuracy
  - Example:
    - Show vetiver functions we have monitoring performance metrics
  - Feedback loops
- Stages of model monitoring maturity (https://www.nannyml.com/blog/5-levels-of-mlops-maturity)
  - Manual
  - Reproducible
  - Automated
    - vetiver has a dashboard template
- You get a model that is easier to maintain (effect)
- For resilient models that are successful in the long term (impact)


## Learn more {background-image="images/image-from-rawpixel-id-8863371-original.jpg" background-opacity=0.4}

-   Documentation at <https://vetiver.rstudio.com/>

-   [Webinar by Isabel Zimmerman and me](https://juliasilge.github.io/mlops-rstudio-meetup/) for Posit Enterprise Meetup

-   End-to-end demos from Posit Solution Engineering in [R](https://github.com/sol-eng/bike_predict) and [Python](https://github.com/sol-eng/bike_predict_python)


## Thank you! {background-image="images/image-from-rawpixel-id-8863371-original.jpg" background-opacity=0.4}

Images by:

::: {.nonincremental}
- [Mohammed Gadi](https://unsplash.com/photos/2qCX0SsSwzs)
- [Albert Edelfelt](https://www.rawpixel.com/image/8863371/image-art-public-domain-woman)
:::

## Thank you! 

<center>

<img src="https://github.com/juliasilge.png" style="border-radius: 50%;" width="300px"/>

[{{< fa brands github >}} \@juliasilge](https://github.com/juliasilge)

[{{< fa brands mastodon >}} \@juliasilge\@fosstodon.org](https://fosstodon.org/@juliasilge)

[{{< fa brands youtube >}} youtube.com/juliasilge](https://www.youtube.com/juliasilge)

[{{< fa link >}} juliasilge.com](https://juliasilge.com/)

</center>
