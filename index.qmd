---
title: 'Reliable maintenance of machine learning models'
format:
  revealjs: 
    footer: <https://juliasilge.github.io/ml-maintenance-2023>
    preview-links: auto
    width: 1280
    height: 720
    incremental: true
    code-line-numbers: false
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"
---

## Hello!

. . .

<center>

<img src="https://github.com/juliasilge.png" style="border-radius: 50%;" width="300px"/>

[{{< fa brands github >}} \@juliasilge](https://github.com/juliasilge)

[{{< fa brands mastodon >}} \@juliasilge\@fosstodon.org](https://fosstodon.org/@juliasilge)

[{{< fa brands youtube >}} youtube.com/juliasilge](https://www.youtube.com/juliasilge)

[{{< fa link >}} juliasilge.com](https://juliasilge.com/)

</center>

#  {background-image="images/mohammed-gadi-2qCX0SsSwzs-unsplash.jpg"}

# Maintaining ML models is NOTHING LIKE THIS  {background-image="images/mohammed-gadi-2qCX0SsSwzs-unsplash.jpg" background-opacity=0.5}

##  {background-image="images/image-from-rawpixel-id-8863371-original.jpg"}


::: {.notes}
Never-ending laundry process

We need to decide when and how to retrain model

We need to measure model performance
:::

# What does performance mean?

::: {.notes}
All software products need to be maintained, but models are both a software artifact *and* a statistical artifact

Unlike something that is "only" a software product, you could, say, retrain your model with new data and change its statistical properties without changing its software properties

Deciding when and how to retrain is part of this model maintenance process, and one of the most basic prerequisites for model maintenance is monitoring the model's performance
:::

## My model is performing well!

. . .

üë©üèº‚Äçüîß My model returns predictions quickly, doesn't use too much memory or processing power, and doesn't have outages.


::: {.callout-note icon=false}

## Metrics

::: {.nonincremental}
- latency
- memory and CPU usage
- uptime
:::


::: {.notes}
"We aren't even talking about the same thing"
:::

## My model is performing well! 

. . .

üë©üèΩ‚Äçüî¨ My model returns predictions that are close to the true values for the predicted quantity.


::: {.callout-note icon=false}

## Metrics

::: {.nonincremental}
- accuracy
- ROC AUC
- F1 score
- RMSE
- log loss
:::

:::

::: {.notes}
As data practitioners, we focus on monitoring statistical performance
:::

# Failures in statistical performance can be **silent** {background-image="images/image-from-rawpixel-id-8863371-original.jpg" background-opacity=0.4}

::: {.notes}
How bad can this be? very bad
:::

# MODEL DRIFT

# DATA DRIFT

# CONCEPT DRIFT

## Outline

- Data drift == statistical distribution of input feature changes
  - About the **inputs**
  - What can you monitor?
    - Statistical distribution of features individually
    - Statistical characteristics of features as a whole
  - True/real changes in distribution vs. changes in data collection or similar  
  - Example: 
    - Show EDA + tidymodels on model trust
- Concept drift == change in the relationship between input features and outcome
  - About the relationship between **inputs** and **outputs**
  - What can you monitor?
    - Model performance metrics, like RMSE or accuracy
  - Example:
    - Show vetiver functions we have monitoring performance metrics

## Feedback loops üîÅ

Deployment of an ML model may *alter* the training data 

. . .

::: {.callout-note icon=false}

## Examples

- Movie recommendation systems on Netflix, Disney+, Hulu
- Identifying fraudulent credit card transactions at Stripe
- Recidivism models

:::


::: footer
[*Building Machine Learning Pipelines* by Hapke & Nelson](https://www.oreilly.com/library/view/building-machine-learning/9781492053187/)
:::

::: {.notes}
Users take some action as a result of a prediction, users rate or correct the quality of a prediction, produce feedback automatically

Example with Chicago data
:::

# Stages of model monitoring maturity


1. Manual üôÇ

2. Reproducible ü§ì

3. Automated ü§©

::: footer
[5 Levels of MLOps Maturity](https://www.nannyml.com/blog/5-levels-of-mlops-maturity)
:::

##

![](images/dashboard1.png){fig-align="center"}

##

![](images/dashboard2.png){fig-align="center"}

##

![](images/dashboard3.png){fig-align="center"}

## Summary

- You get a model that is easier to maintain (effect)
- For resilient models that are successful in the long term (impact)


## Learn more {background-image="images/image-from-rawpixel-id-8863371-original.jpg" background-opacity=0.4}

::: {.nonincremental}
-   Documentation at <https://vetiver.rstudio.com/>

-   [Webinar by Isabel Zimmerman and me](https://juliasilge.github.io/mlops-rstudio-meetup/) for Posit Enterprise Meetup

-   End-to-end demos from Posit Solution Engineering in [R](https://github.com/sol-eng/bike_predict) and [Python](https://github.com/sol-eng/bike_predict_python)
:::


## Thank you! {background-image="images/image-from-rawpixel-id-8863371-original.jpg" background-opacity=0.4}

<center>

<img src="https://github.com/juliasilge.png" style="border-radius: 50%;" width="300px"/>

[{{< fa brands github >}} \@juliasilge](https://github.com/juliasilge)

[{{< fa brands mastodon >}} \@juliasilge\@fosstodon.org](https://fosstodon.org/@juliasilge)

[{{< fa brands youtube >}} youtube.com/juliasilge](https://www.youtube.com/juliasilge)

[{{< fa link >}} juliasilge.com](https://juliasilge.com/)

</center>

::: footer
Image credits: [Mohammed Gadi](https://unsplash.com/photos/2qCX0SsSwzs) & [Albert Edelfelt](https://www.rawpixel.com/image/8863371/image-art-public-domain-woman)
:::
